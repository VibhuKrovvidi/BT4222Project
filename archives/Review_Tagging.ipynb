{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7fa50a224290>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import deplacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scraped_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Restaurant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(Translated by Google) Good hamburgers, delici...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Burger was ok, not fantastic.  Place was extre...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>3 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>One of the better burgers I've had, the bread ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review         Date  \\\n",
       "0           0  (Translated by Google) Good hamburgers, delici...   5 days ago   \n",
       "3           3  Burger was ok, not fantastic.  Place was extre...  2 weeks ago   \n",
       "4           4  One of the better burgers I've had, the bread ...  2 weeks ago   \n",
       "\n",
       "       Stars Restaurant  \n",
       "0   4 stars        TFDB  \n",
       "3   3 stars        TFDB  \n",
       "4   5 stars        TFDB  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x7fa509b6bd10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import deplacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onion NOUN <╗   compound\n",
      "rings NOUN ═╝<╗ nsubj\n",
      "were  AUX  ═╗═╝ ROOT\n",
      "good  ADJ  <╝   acomp\n",
      "[]\n",
      "{'onion': [], 'rings': ['good']}\n"
     ]
    }
   ],
   "source": [
    "### REFERENCE\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import deplacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')\n",
    "\n",
    "#Text 1 - With Punctuation and conj at each sentiment split\n",
    "#Text 2 - With only 1 conjuction\n",
    "#Text 3 - With no conj so we are only looking at the noun_dict\n",
    "\n",
    "# text = \"In my opinion, the food was really really good , but their ambiance was shit, and my mom thinks the waiter was amazing\"\n",
    "# text = \"In my opinion, the food was really really good but their ambiance was shit, my mom thinks the waiter was amazing\"\n",
    "text = \"onion rings were good\"\n",
    "\n",
    "doc = en(text)\n",
    "deplacy.render(doc)\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "dict_noun = {} # This is for the dict of Noun - Adverb, adj, verb\n",
    "\n",
    "for sent in doc.sents:\n",
    "    #calculate the dict_noun first\n",
    "\n",
    "    for word in sent:\n",
    "            if word.pos_ == 'NOUN':\n",
    "                current_noun = word.text\n",
    "                dict_noun[current_noun] = []\n",
    "            if word.pos_ == 'ADV' or word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "                dict_noun[current_noun].append(word.text)\n",
    "\n",
    "    #See if there are conjunctions to split on. If so get the heads for the subtrees\n",
    "    heads = [cc for cc in sent if cc.dep_ == 'conj']\n",
    "    print(heads)\n",
    "\n",
    "    # If there are no conjunctions split only on punctuation.\n",
    "    if(len(heads)==0):\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in sent:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array]))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array]))\n",
    "        chunks.append( (counter, chunk) )\n",
    "    else:\n",
    "        for head in heads:\n",
    "            words = [None]*100\n",
    "            counter = 0\n",
    "            for i in head.subtree:\n",
    "                if(i in heads):\n",
    "                    if(i != head):\n",
    "                        break\n",
    "                if(i.pos_ == 'PUNCT'):\n",
    "                    break\n",
    "                words[counter] = i\n",
    "                counter = counter + 1\n",
    "                if(i.dep_ == 'cc'):\n",
    "                    break\n",
    "            res = []\n",
    "            for val in words:\n",
    "                if val != None :\n",
    "                    res.append(val)\n",
    "            for word in res:\n",
    "                seen.add(word)\n",
    "            chunk = (' '.join([ww.text for ww in res]))\n",
    "            chunks.append( (head.i, chunk) )\n",
    "        unseen = [ww for ww in sent if ww not in seen]\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in unseen:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "        chunks.append( (counter, chunk) )\n",
    "#         chunk = ' '.join([ww.text for ww in unseen if ww.dep_ != 'cc' and ww.pos_ != 'PUNCT'])\n",
    "#         chunks.append( (sent.root.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "print(dict_noun)\n",
    "# for ii, chunk in chunks:\n",
    "#     if (len(chunk) != 0):\n",
    "#         print(\"The chunk is: \" + chunk)\n",
    "#     doc = en(chunk)\n",
    "#     for i in doc.sents:\n",
    "#         for cc in i:\n",
    "#             if cc.dep_ == 'nsubj':\n",
    "#                 continue\n",
    "#                 print(\"Noun :\" + cc.text)\n",
    "#     if(doc._.polarity != 0.0):\n",
    "#         print(\"Polarity: \" + str(doc._.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do feature-descriptor extraction from each row:\n",
    "def get_feature_descriptors(text):\n",
    "#     text = row[\"text\"]\n",
    "\n",
    "    doc = en(text)\n",
    "#     deplacy.render(doc)\n",
    "    seen = set() # keep track of covered words\n",
    "    dict_noun = {} # This is for the dict of Noun - Adverb, adj, verb\n",
    "    chunks = []\n",
    "    for sent in doc.sents:\n",
    "    #calculate the dict_noun first\n",
    "\n",
    "        for word in sent:\n",
    "                \n",
    "                if word.pos_ == 'NOUN':\n",
    "                    current_noun = word.text\n",
    "                    dict_noun[current_noun] = []\n",
    "                if word.pos_ == 'ADV' or word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "                    try:\n",
    "                        dict_noun[current_noun].append(word.text)\n",
    "                    except:\n",
    "                        pass;\n",
    "                    \n",
    "\n",
    "        #See if there are conjunctions to split on. If so get the heads for the subtrees\n",
    "        heads = [cc for cc in sent if cc.dep_ == 'conj']\n",
    "#         print(heads)\n",
    "\n",
    "        # If there are no conjunctions split only on punctuation.\n",
    "        if(len(heads)==0):\n",
    "            counter = 0\n",
    "            array = []\n",
    "            for word in sent:\n",
    "                if word.pos_ != 'PUNCT':\n",
    "                    array.append(word)\n",
    "                    continue\n",
    "                chunk = (' '.join([ww.text for ww in array]))\n",
    "                chunks.append( (counter, chunk) )\n",
    "                counter = counter + 1\n",
    "                array = []\n",
    "            chunk = (' '.join([ww.text for ww in array]))\n",
    "            chunks.append( (counter, chunk) )\n",
    "        else:\n",
    "            for head in heads:\n",
    "                words = [None]*100\n",
    "                counter = 0\n",
    "                for i in head.subtree:\n",
    "                    if(i in heads):\n",
    "                        if(i != head):\n",
    "                            break\n",
    "                    if(i.pos_ == 'PUNCT'):\n",
    "                        break\n",
    "                    words[counter] = i\n",
    "                    counter = counter + 1\n",
    "                    if(i.dep_ == 'cc'):\n",
    "                        break\n",
    "                res = []\n",
    "                for val in words:\n",
    "                    if val != None :\n",
    "                        res.append(val)\n",
    "                for word in res:\n",
    "                    seen.add(word)\n",
    "                chunk = (' '.join([ww.text for ww in res]))\n",
    "                chunks.append( (head.i, chunk) )\n",
    "            unseen = [ww for ww in sent if ww not in seen]\n",
    "            counter = 0\n",
    "            array = []\n",
    "            for word in unseen:\n",
    "                if word.pos_ != 'PUNCT':\n",
    "                    array.append(word)\n",
    "                    continue\n",
    "                chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "                chunks.append( (counter, chunk) )\n",
    "                counter = counter + 1\n",
    "                array = []\n",
    "            chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "            chunks.append( (counter, chunk) )\n",
    "    return dict_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trialdf = df.copy()\n",
    "trialdf = df#.sample(100, random_state=4222)\n",
    "# trialdf\n",
    "\n",
    "trialdf[\"feature_descriptors\"] = trialdf[\"Review\"].apply(lambda x:get_feature_descriptors(x))\n",
    "\n",
    "# for i in tqdm(range(0, len(df))):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burgers': ['reminds', 'only'],\n",
       " 'thing': ['missing', 'perfect'],\n",
       " 'weather': ['Fantastic'],\n",
       " 'char': [],\n",
       " 'patty': [],\n",
       " 'onion': [],\n",
       " 'rings': ['nicely', 'done']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trialdf.iloc[5, :][\"feature_descriptors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trialdf.to_csv(\"featured_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\vibkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "benepar.download('benepar_en3')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "doc = nlp('The time for action is now. It is never too late to do something.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
      "('S',)\n",
      "[The time for action, is now, .]\n",
      "(S (NP (PRP It)) (VP (VBZ is) (ADVP (RB never)) (ADJP (RB too) (JJ late) (S (VP (TO to) (VP (VB do) (NP (NN something))))))) (. .))\n",
      "('S',)\n",
      "[It, is never too late to do something, .]\n"
     ]
    }
   ],
   "source": [
    "subclauses = []\n",
    "for sent in list(doc.sents):\n",
    "    print(sent._.parse_string)\n",
    "    print(sent._.labels)\n",
    "    print(list(sent._.children))\n",
    "    subclauses.append(list(sent._.children))\n",
    "\n",
    "subclauses = [item for sublist in subclauses for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The time for action, is now, ., It, is never too late to do something, .]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subclauses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

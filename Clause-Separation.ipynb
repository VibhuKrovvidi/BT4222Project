{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The burger was shit, however the food was alright\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \t 0 \t DET \t det \t ['burger', 'was', 'was'] \t []\n",
      "burger \t 1 \t NOUN \t nsubj \t ['was', 'was'] \t ['The']\n",
      "was \t 2 \t AUX \t ccomp \t ['was'] \t ['burger', 'shit']\n",
      "shit \t 3 \t ADJ \t attr \t ['was', 'was'] \t []\n",
      ", \t 4 \t PUNCT \t punct \t ['was'] \t []\n",
      "however \t 5 \t ADV \t advmod \t ['was'] \t []\n",
      "the \t 6 \t DET \t det \t ['food', 'was'] \t []\n",
      "food \t 7 \t NOUN \t nsubj \t ['was'] \t ['the']\n",
      "was \t 8 \t AUX \t ROOT \t [] \t ['was', ',', 'however', 'food', 'alright']\n",
      "alright \t 9 \t ADJ \t acomp \t ['was'] \t []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    ancestors = [t.text for t in token.ancestors]\n",
    "    children = [t.text for t in token.children]\n",
    "    print(token.text, \"\\t\", token.i, \"\\t\", \n",
    "          token.pos_, \"\\t\", token.dep_, \"\\t\", \n",
    "          ancestors, \"\\t\", children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_of_sentence(doc):\n",
    "    root_token = None\n",
    "    for token in doc:\n",
    "        if (token.dep_ == \"ROOT\"):\n",
    "            root_token = token\n",
    "    return root_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_token = find_root_of_sentence(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_other_verbs(doc, root_token):\n",
    "    other_verbs = []\n",
    "    for token in doc:\n",
    "        ancestors = list(token.ancestors)\n",
    "        if (token.pos_ == \"VERB\" and len(ancestors) == 1\\\n",
    "            and ancestors[0] == root_token):\n",
    "            other_verbs.append(token)\n",
    "    return other_verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_verbs = find_other_verbs(doc, root_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clause_token_span_for_verb(verb, doc, all_verbs):\n",
    "    first_token_index = len(doc)\n",
    "    last_token_index = 0\n",
    "    this_verb_children = list(verb.children)\n",
    "    for child in this_verb_children:\n",
    "        if (child not in all_verbs):\n",
    "            if (child.i < first_token_index):\n",
    "                first_token_index = child.i\n",
    "            if (child.i > last_token_index):\n",
    "                last_token_index = child.i\n",
    "    return(first_token_index, last_token_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_spans = []   \n",
    "all_verbs = [root_token] + other_verbs\n",
    "for other_verb in all_verbs:\n",
    "    (first_token_index, last_token_index) = \\\n",
    "     get_clause_token_span_for_verb(other_verb, \n",
    "                                    doc, all_verbs)\n",
    "    token_spans.append((first_token_index, \n",
    "                        last_token_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_clauses = []\n",
    "for token_span in token_spans:\n",
    "    start = token_span[0]\n",
    "    end = token_span[1]\n",
    "    if (start < end):\n",
    "        clause = doc[start:end]\n",
    "        sentence_clauses.append(clause)\n",
    "sentence_clauses = sorted(sentence_clauses, \n",
    "                          key=lambda tup: tup[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was shit, however the food was']\n"
     ]
    }
   ],
   "source": [
    "clauses_text = [clause.text for clause in sentence_clauses]\n",
    "print(clauses_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In       ADP   ═══╗<════╗       prep\n",
      "my       PRON  <╗ ║     ║       poss\n",
      "opinion  NOUN  ═╝<╝     ║       pobj\n",
      ",        PUNCT <════════║═╗     punct\n",
      "the      DET   <╗       ║ ║     det\n",
      "food     NOUN  ═╝<════╗ ║ ║     nsubj\n",
      "was      AUX   ═╗═╗═╗═╝═╝═╝═╗═╗ ROOT\n",
      "really   ADV   <╝ ║ ║       ║ ║ advmod\n",
      "really   ADV   <╗ ║ ║       ║ ║ advmod\n",
      "good     ADJ   ═╝<╝ ║       ║ ║ acomp\n",
      ",        PUNCT <════║═══════╝ ║ punct\n",
      "but      CCONJ <════╝         ║ cc\n",
      "their    PRON  <╗             ║ poss\n",
      "ambiance NOUN  ═╝<══╗         ║ nsubj\n",
      "was      VERB  ═╗═╗═╝═╗═╗<════╝ conj\n",
      "shit     VERB  <╝ ║   ║ ║       acomp\n",
      ",        PUNCT <══║═══╝ ║       punct\n",
      "and      CCONJ <══╝     ║       cc\n",
      "my       PRON  <╗       ║       poss\n",
      "mom      NOUN  ═╝<════╗ ║       nsubj\n",
      "thinks   VERB  ═════╗═╝<╝       conj\n",
      "the      DET   <╗   ║           det\n",
      "waiter   NOUN  ═╝<╗ ║           nsubj\n",
      "was      AUX   ═╗═╝<╝           ccomp\n",
      "amazing  ADJ   <╝               acomp\n",
      "[was, thinks]\n",
      "{'opinion': [], 'food': ['amazing', 'amazing', 'amazing'], 'ambiance': ['amazing', 'amazing'], 'mom': ['amazing'], 'waiter': ['amazing']}\n",
      "The chunk is: In my opinion\n",
      "The chunk is: the food was really really good\n",
      "Polarity: 0.7\n",
      "The chunk is: their ambiance was shit\n",
      "Polarity: -0.2\n",
      "The chunk is: my mom thinks the waiter was amazing\n",
      "Polarity: 0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import deplacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')\n",
    "\n",
    "#Text 1 - With Punctuation and conj at each sentiment split\n",
    "#Text 2 - With only 1 conjuction\n",
    "#Text 3 - With no conj so we are only looking at the noun_dict\n",
    "\n",
    "text = \"In my opinion, the food was really really good , but their ambiance was shit, and my mom thinks the waiter was amazing\"\n",
    "# text = \"In my opinion, the food was really really good but their ambiance was shit, my mom thinks the waiter was amazing\"\n",
    "# text = \"In my opinion, the food was really really good, their ambiance was shit, my mom thinks the waiter was amazing\"\n",
    "\n",
    "doc = en(text)\n",
    "deplacy.render(doc)\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "dict_noun = {} # This is for the dict of Noun - Adverb, adj, verb\n",
    "\n",
    "for sent in doc.sents:\n",
    "    #calculate the dict_noun first\n",
    "\n",
    "    for word in sent:\n",
    "            if word.pos_ == 'NOUN':\n",
    "                current_noun = word.text\n",
    "                dict_noun[current_noun] = []\n",
    "            if word.pos_ == 'ADV' or word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "                dict_noun[current_noun].append(lol.text)\n",
    "\n",
    "    #See if there are conjunctions to split on. If so get the heads for the subtrees\n",
    "    heads = [cc for cc in sent if cc.dep_ == 'conj']\n",
    "    print(heads)\n",
    "\n",
    "    # If there are no conjunctions split only on punctuation.\n",
    "    if(len(heads)==0):\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in sent:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array]))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array]))\n",
    "        chunks.append( (counter, chunk) )\n",
    "    else:\n",
    "        for head in heads:\n",
    "            words = [None]*100\n",
    "            counter = 0\n",
    "            for i in head.subtree:\n",
    "                if(i in heads):\n",
    "                    if(i != head):\n",
    "                        break\n",
    "                if(i.pos_ == 'PUNCT'):\n",
    "                    break\n",
    "                words[counter] = i\n",
    "                counter = counter + 1\n",
    "                if(i.dep_ == 'cc'):\n",
    "                    break\n",
    "            res = []\n",
    "            for val in words:\n",
    "                if val != None :\n",
    "                    res.append(val)\n",
    "            for word in res:\n",
    "                seen.add(word)\n",
    "            chunk = (' '.join([ww.text for ww in res]))\n",
    "            chunks.append( (head.i, chunk) )\n",
    "        unseen = [ww for ww in sent if ww not in seen]\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in unseen:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "        chunks.append( (counter, chunk) )\n",
    "#         chunk = ' '.join([ww.text for ww in unseen if ww.dep_ != 'cc' and ww.pos_ != 'PUNCT'])\n",
    "#         chunks.append( (sent.root.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "print(dict_noun)\n",
    "for ii, chunk in chunks:\n",
    "    if (len(chunk) != 0):\n",
    "        print(\"The chunk is: \" + chunk)\n",
    "    doc = en(chunk)\n",
    "    for i in doc.sents:\n",
    "        for cc in i:\n",
    "            if cc.dep_ == 'nsubj':\n",
    "                continue\n",
    "                print(\"Noun :\" + cc.text)\n",
    "    if(doc._.polarity != 0.0):\n",
    "        print(\"Polarity: \" + str(doc._.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

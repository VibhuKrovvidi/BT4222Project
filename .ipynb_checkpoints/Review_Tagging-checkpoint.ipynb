{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scraped_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suitable-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Restaurant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(Translated by Google) Good hamburgers, delici...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Burger was ok, not fantastic.  Place was extre...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>3 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>One of the better burgers I've had, the bread ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>TFDB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review         Date  \\\n",
       "0           0  (Translated by Google) Good hamburgers, delici...   5 days ago   \n",
       "3           3  Burger was ok, not fantastic.  Place was extre...  2 weeks ago   \n",
       "4           4  One of the better burgers I've had, the bread ...  2 weeks ago   \n",
       "\n",
       "       Stars Restaurant  \n",
       "0   4 stars        TFDB  \n",
       "3   3 stars        TFDB  \n",
       "4   5 stars        TFDB  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "further-capture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacytextblob.spacytextblob.SpacyTextBlob at 0x1c244edaa60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import deplacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "arranged-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onion NOUN <╗   compound\n",
      "rings NOUN ═╝<╗ nsubj\n",
      "were  AUX  ═╗═╝ ROOT\n",
      "good  ADJ  <╝   acomp\n",
      "[]\n",
      "{'onion': [], 'rings': ['good']}\n"
     ]
    }
   ],
   "source": [
    "### REFERENCE\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import deplacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "en.add_pipe('spacytextblob')\n",
    "\n",
    "#Text 1 - With Punctuation and conj at each sentiment split\n",
    "#Text 2 - With only 1 conjuction\n",
    "#Text 3 - With no conj so we are only looking at the noun_dict\n",
    "\n",
    "# text = \"In my opinion, the food was really really good , but their ambiance was shit, and my mom thinks the waiter was amazing\"\n",
    "# text = \"In my opinion, the food was really really good but their ambiance was shit, my mom thinks the waiter was amazing\"\n",
    "text = \"onion rings were good\"\n",
    "\n",
    "doc = en(text)\n",
    "deplacy.render(doc)\n",
    "\n",
    "seen = set() # keep track of covered words\n",
    "\n",
    "chunks = []\n",
    "dict_noun = {} # This is for the dict of Noun - Adverb, adj, verb\n",
    "\n",
    "for sent in doc.sents:\n",
    "    #calculate the dict_noun first\n",
    "\n",
    "    for word in sent:\n",
    "            if word.pos_ == 'NOUN':\n",
    "                current_noun = word.text\n",
    "                dict_noun[current_noun] = []\n",
    "            if word.pos_ == 'ADV' or word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "                dict_noun[current_noun].append(word.text)\n",
    "\n",
    "    #See if there are conjunctions to split on. If so get the heads for the subtrees\n",
    "    heads = [cc for cc in sent if cc.dep_ == 'conj']\n",
    "    print(heads)\n",
    "\n",
    "    # If there are no conjunctions split only on punctuation.\n",
    "    if(len(heads)==0):\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in sent:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array]))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array]))\n",
    "        chunks.append( (counter, chunk) )\n",
    "    else:\n",
    "        for head in heads:\n",
    "            words = [None]*100\n",
    "            counter = 0\n",
    "            for i in head.subtree:\n",
    "                if(i in heads):\n",
    "                    if(i != head):\n",
    "                        break\n",
    "                if(i.pos_ == 'PUNCT'):\n",
    "                    break\n",
    "                words[counter] = i\n",
    "                counter = counter + 1\n",
    "                if(i.dep_ == 'cc'):\n",
    "                    break\n",
    "            res = []\n",
    "            for val in words:\n",
    "                if val != None :\n",
    "                    res.append(val)\n",
    "            for word in res:\n",
    "                seen.add(word)\n",
    "            chunk = (' '.join([ww.text for ww in res]))\n",
    "            chunks.append( (head.i, chunk) )\n",
    "        unseen = [ww for ww in sent if ww not in seen]\n",
    "        counter = 0\n",
    "        array = []\n",
    "        for word in unseen:\n",
    "            if word.pos_ != 'PUNCT':\n",
    "                array.append(word)\n",
    "                continue\n",
    "            chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "            chunks.append( (counter, chunk) )\n",
    "            counter = counter + 1\n",
    "            array = []\n",
    "        chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "        chunks.append( (counter, chunk) )\n",
    "#         chunk = ' '.join([ww.text for ww in unseen if ww.dep_ != 'cc' and ww.pos_ != 'PUNCT'])\n",
    "#         chunks.append( (sent.root.i, chunk) )\n",
    "\n",
    "chunks = sorted(chunks, key=lambda x: x[0])\n",
    "print(dict_noun)\n",
    "# for ii, chunk in chunks:\n",
    "#     if (len(chunk) != 0):\n",
    "#         print(\"The chunk is: \" + chunk)\n",
    "#     doc = en(chunk)\n",
    "#     for i in doc.sents:\n",
    "#         for cc in i:\n",
    "#             if cc.dep_ == 'nsubj':\n",
    "#                 continue\n",
    "#                 print(\"Noun :\" + cc.text)\n",
    "#     if(doc._.polarity != 0.0):\n",
    "#         print(\"Polarity: \" + str(doc._.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "generic-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge double occurring nouns:\n",
    "def merge_double_nouns(text):\n",
    "    doc = en(text)\n",
    "    for sent in doc.sents:\n",
    "        for i in range(0, len(sent)):\n",
    "#             try:\n",
    "            if sent[i].pos_ == \"NOUN\" and sent[i+1].pos_ == \"NOUN\":\n",
    "                st = sent[i] + \" \" + sent[i+1]\n",
    "                text.replace(st, sent[i] + sent[i+1])\n",
    "#             except:\n",
    "#                 pass;\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "blank-midnight",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'spacy.tokens.token.Token' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-49d3b42a765c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtry2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmerge_double_nouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\vibkr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4133\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4134\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4135\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-49d3b42a765c>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtry2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmerge_double_nouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-efc957b30485>\u001b[0m in \u001b[0;36mmerge_double_nouns\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#             try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NOUN\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"NOUN\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                 \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#             except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'spacy.tokens.token.Token' and 'str'"
     ]
    }
   ],
   "source": [
    "try2 = df[\"Review\"].apply(lambda x:merge_double_nouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "synthetic-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great\n",
      "burgers\n",
      "that\n",
      "reminds\n",
      "you\n",
      "of\n",
      "LA.\n",
      "The\n",
      "only\n",
      "thing\n",
      "missing\n",
      "is\n",
      "the\n",
      "perfect\n",
      "weather.\n",
      "Fantastic\n",
      "char\n",
      "to\n",
      "the\n",
      "patty\n",
      "and\n",
      "the\n",
      "onion\n",
      "rings\n",
      "were\n",
      "nicely\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for word in try2.iloc[5].split(' '):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "original-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do feature-descriptor extraction from each row:\n",
    "def get_feature_descriptors(text):\n",
    "#     text = row[\"text\"]\n",
    "\n",
    "    doc = en(text)\n",
    "#     deplacy.render(doc)\n",
    "    seen = set() # keep track of covered words\n",
    "    dict_noun = {} # This is for the dict of Noun - Adverb, adj, verb\n",
    "    chunks = []\n",
    "    for sent in doc.sents:\n",
    "    #calculate the dict_noun first\n",
    "\n",
    "        for word in sent:\n",
    "                \n",
    "                if word.pos_ == 'NOUN':\n",
    "                    current_noun = word.text\n",
    "                    dict_noun[current_noun] = []\n",
    "                if word.pos_ == 'ADV' or word.pos_ == 'ADJ' or word.pos_ == 'VERB':\n",
    "                    try:\n",
    "                        dict_noun[current_noun].append(word.text)\n",
    "                    except:\n",
    "                        pass;\n",
    "                    \n",
    "\n",
    "        #See if there are conjunctions to split on. If so get the heads for the subtrees\n",
    "        heads = [cc for cc in sent if cc.dep_ == 'conj']\n",
    "#         print(heads)\n",
    "\n",
    "        # If there are no conjunctions split only on punctuation.\n",
    "        if(len(heads)==0):\n",
    "            counter = 0\n",
    "            array = []\n",
    "            for word in sent:\n",
    "                if word.pos_ != 'PUNCT':\n",
    "                    array.append(word)\n",
    "                    continue\n",
    "                chunk = (' '.join([ww.text for ww in array]))\n",
    "                chunks.append( (counter, chunk) )\n",
    "                counter = counter + 1\n",
    "                array = []\n",
    "            chunk = (' '.join([ww.text for ww in array]))\n",
    "            chunks.append( (counter, chunk) )\n",
    "        else:\n",
    "            for head in heads:\n",
    "                words = [None]*100\n",
    "                counter = 0\n",
    "                for i in head.subtree:\n",
    "                    if(i in heads):\n",
    "                        if(i != head):\n",
    "                            break\n",
    "                    if(i.pos_ == 'PUNCT'):\n",
    "                        break\n",
    "                    words[counter] = i\n",
    "                    counter = counter + 1\n",
    "                    if(i.dep_ == 'cc'):\n",
    "                        break\n",
    "                res = []\n",
    "                for val in words:\n",
    "                    if val != None :\n",
    "                        res.append(val)\n",
    "                for word in res:\n",
    "                    seen.add(word)\n",
    "                chunk = (' '.join([ww.text for ww in res]))\n",
    "                chunks.append( (head.i, chunk) )\n",
    "            unseen = [ww for ww in sent if ww not in seen]\n",
    "            counter = 0\n",
    "            array = []\n",
    "            for word in unseen:\n",
    "                if word.pos_ != 'PUNCT':\n",
    "                    array.append(word)\n",
    "                    continue\n",
    "                chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "                chunks.append( (counter, chunk) )\n",
    "                counter = counter + 1\n",
    "                array = []\n",
    "            chunk = (' '.join([ww.text for ww in array if ww.dep_ != 'cc']))\n",
    "            chunks.append( (counter, chunk) )\n",
    "    return dict_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "generous-novelty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trialdf = df.copy()\n",
    "trialdf = df#.sample(100, random_state=4222)\n",
    "# trialdf\n",
    "\n",
    "trialdf[\"feature_descriptors\"] = trialdf[\"Review\"].apply(lambda x:get_feature_descriptors(x))\n",
    "\n",
    "# for i in tqdm(range(0, len(df))):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "closed-bearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burgers': ['reminds', 'only'],\n",
       " 'thing': ['missing', 'perfect'],\n",
       " 'weather': ['Fantastic'],\n",
       " 'char': ['patty'],\n",
       " 'onion': [],\n",
       " 'rings': ['nicely', 'done']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trialdf.iloc[5, :][\"feature_descriptors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "behind-gentleman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Date</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>feature_descriptors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(Translated by Google) Good hamburgers, delici...</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>TFDB</td>\n",
       "      <td>{'hamburgers': ['Good hamburgers , .'], 'cockt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Burger was ok, not fantastic.  Place was extre...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>3 stars</td>\n",
       "      <td>TFDB</td>\n",
       "      <td>{'night': ['  Place was extremely hot even tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>One of the better burgers I've had, the bread ...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>TFDB</td>\n",
       "      <td>{'burgers': ['One of the better burgers I 've ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Diner time and date: 8 October 2021, 1pm. My w...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>TFDB</td>\n",
       "      <td>{'time': ['Diner time : 8 October 2021 , 1 pm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Excellent burgers with a taste of each ingredi...</td>\n",
       "      <td>2 weeks ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>TFDB</td>\n",
       "      <td>{'burgers': ['Excellent burgers with a taste o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>253</td>\n",
       "      <td>Nice little Italian place with warm and family...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>Art</td>\n",
       "      <td>{'place': ['Nice little Italian place with war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>255</td>\n",
       "      <td>Classy Italian\\n\\nGreat Italian fare in a clas...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>Art</td>\n",
       "      <td>{'fare': ['Classy Italian \n",
       "\n",
       " Great Italian far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>256</td>\n",
       "      <td>Best home made pasta ever</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>Art</td>\n",
       "      <td>{'home': ['Best home made pasta ever'], 'pasta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>258</td>\n",
       "      <td>Classic Italian restaurant in Raffles Place. M...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>Art</td>\n",
       "      <td>{'restaurant': ['Classic Italian restaurant in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>259</td>\n",
       "      <td>Very good pasta. Their dishes have a unique tw...</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>3 stars</td>\n",
       "      <td>Art</td>\n",
       "      <td>{'pasta': ['Very good pasta .'], 'dishes': ['T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2794 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             Review  \\\n",
       "0              0  (Translated by Google) Good hamburgers, delici...   \n",
       "3              3  Burger was ok, not fantastic.  Place was extre...   \n",
       "4              4  One of the better burgers I've had, the bread ...   \n",
       "5              5  Diner time and date: 8 October 2021, 1pm. My w...   \n",
       "6              6  Excellent burgers with a taste of each ingredi...   \n",
       "...          ...                                                ...   \n",
       "4230         253  Nice little Italian place with warm and family...   \n",
       "4232         255  Classy Italian\\n\\nGreat Italian fare in a clas...   \n",
       "4233         256                          Best home made pasta ever   \n",
       "4235         258  Classic Italian restaurant in Raffles Place. M...   \n",
       "4236         259  Very good pasta. Their dishes have a unique tw...   \n",
       "\n",
       "             Date      Stars Restaurant  \\\n",
       "0      5 days ago   4 stars        TFDB   \n",
       "3     2 weeks ago   3 stars        TFDB   \n",
       "4     2 weeks ago   5 stars        TFDB   \n",
       "5     2 weeks ago   5 stars        TFDB   \n",
       "6     2 weeks ago   5 stars        TFDB   \n",
       "...           ...        ...        ...   \n",
       "4230  5 years ago   5 stars         Art   \n",
       "4232  5 years ago   4 stars         Art   \n",
       "4233  5 years ago   4 stars         Art   \n",
       "4235  5 years ago   5 stars         Art   \n",
       "4236  5 years ago   3 stars         Art   \n",
       "\n",
       "                                    feature_descriptors  \n",
       "0     {'hamburgers': ['Good hamburgers , .'], 'cockt...  \n",
       "3     {'night': ['  Place was extremely hot even tho...  \n",
       "4     {'burgers': ['One of the better burgers I 've ...  \n",
       "5     {'time': ['Diner time : 8 October 2021 , 1 pm ...  \n",
       "6     {'burgers': ['Excellent burgers with a taste o...  \n",
       "...                                                 ...  \n",
       "4230  {'place': ['Nice little Italian place with war...  \n",
       "4232  {'fare': ['Classy Italian \n",
       "\n",
       " Great Italian far...  \n",
       "4233  {'home': ['Best home made pasta ever'], 'pasta...  \n",
       "4235  {'restaurant': ['Classic Italian restaurant in...  \n",
       "4236  {'pasta': ['Very good pasta .'], 'dishes': ['T...  \n",
       "\n",
       "[2794 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trialdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-princeton",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "shared-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\vibkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "benepar.download('benepar_en3')\n",
    "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "doc = nlp('The time for action is now. It is never too late to do something.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sitting-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NP (DT The) (NN time)) (PP (IN for) (NP (NN action)))) (VP (VBZ is) (ADVP (RB now))) (. .))\n",
      "('S',)\n",
      "[The time for action, is now, .]\n",
      "(S (NP (PRP It)) (VP (VBZ is) (ADVP (RB never)) (ADJP (RB too) (JJ late) (S (VP (TO to) (VP (VB do) (NP (NN something))))))) (. .))\n",
      "('S',)\n",
      "[It, is never too late to do something, .]\n"
     ]
    }
   ],
   "source": [
    "subclauses = []\n",
    "for sent in list(doc.sents):\n",
    "    print(sent._.parse_string)\n",
    "    print(sent._.labels)\n",
    "    print(list(sent._.children))\n",
    "    subclauses.append(list(sent._.children))\n",
    "\n",
    "subclauses = [item for sublist in subclauses for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boxed-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The time for action, is now, ., It, is never too late to do something, .]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subclauses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
